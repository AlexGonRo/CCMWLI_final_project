{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip, pickle as cPickle\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# For Tensorflow & Keras\n",
    "import tensorflow as tf\n",
    "import keras.layers as lyr\n",
    "from keras.models import Model\n",
    "#For Padding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#For XGBoost\n",
    "import xgboost as xgb\n",
    "#For Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "#from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating stopwords\n",
      "SPLIT DATA\n",
      "LOAD TRAIN DATA\n",
      "LOAD TEST DATA\n",
      "FIT CORPUS ON COUNT VECTORIZER\n"
     ]
    }
   ],
   "source": [
    "# DEFINE GLOBAL VALUES\n",
    "STOPS = prep_stops()\n",
    "Train_Path = './input/df10.csv' # JUST TO TEST FASTER , normally train.csv\n",
    "Test_Path = './input/test1000.csv'\n",
    "#Train_Path = './input/train.csv'\n",
    "#Test_Path = './input/test.csv'\n",
    "num_voc = 5000\n",
    "EMBEDDING_DIM=300\n",
    "lstm_units = 256 # Lower if network takes too long\n",
    "max_seq_length = 10\n",
    "num_dense = 100\n",
    "rate_drop_lstm = 0.3\n",
    "rate_drop_dense = 0.25\n",
    "\n",
    "\n",
    "\n",
    "# LOAD DATA\n",
    "X_train, X_traintest, Y_train, Y_traintest = splitData()\n",
    "X_test, test_idxs = testData()\n",
    "\n",
    "# 1st Stage, Question to Int Array\n",
    "# Build Vocabulary\n",
    "\n",
    "f1 = BoW()\n",
    "f1.fit(X_train.append(X_traintest), num_voc)\n",
    "X_all = X_train.append(X_traintest)\n",
    "mycv = f1.getVectorizer()\n",
    "#words to tokens\n",
    "X_test_q1 = create_padded_seqs(X_test.question1,mycv, max_seq_length)\n",
    "X_test_q2 = create_padded_seqs(X_test.question2, mycv, max_seq_length)\n",
    "X_train_q1 = create_padded_seqs(X_train.question1, mycv, max_seq_length)\n",
    "X_train_q2 = create_padded_seqs(X_train.question2, mycv, max_seq_length)\n",
    "X_traintest_q1 = create_padded_seqs(X_traintest.question1, mycv, max_seq_length)\n",
    "X_traintest_q2 = create_padded_seqs(X_traintest.question2, mycv, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 54\n"
     ]
    }
   ],
   "source": [
    "# Create Embedding Matrix: Voc Entity --> Vector\n",
    "MODEL_FILE = './input/300features_40minwords_10context.bin'\n",
    "#model = Word2Vec.load(EMBEDDING_FILE)\n",
    "model = Word2Vec.load(MODEL_FILE)\n",
    "vocab = model.wv.vocab\n",
    "#word2vec = KeyedVectors.load_word2vec_format(MODEL_FILE, \\\n",
    "#        binary=True)\n",
    "print \"Loaded model\"\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = num_voc\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in mycv.vocabulary_.iteritems():\n",
    "    if word in vocab:\n",
    "        embedding_matrix[i] = model[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 300)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD TRAIN DATA\n"
     ]
    }
   ],
   "source": [
    "# LOAD VECTORS AS INPUT ---> DOESN'T WORK WITH EMBEDDING LAYER\n",
    "# question1 = embedding[0:len(embedding)/2]\n",
    "# question2 = embedding[len(embedding)/2:]\n",
    "# idx = range(0,len(question1))\n",
    "# # 3rd Stage LOAD THE MEAN-VECTORS\n",
    "# EMBEDDING_FILE = './input/arr0_1000.npy'\n",
    "# embedding = load_zipped_pickle(EMBEDDING_FILE)\n",
    "# LOAD DATA for Labels\n",
    "# Train_Path = './input/df10.csv' # JUST TO TEST FASTER , normally train.csv\n",
    "# Test_Path = './input/test1000.csv'\n",
    "# train = load_train()\n",
    "\n",
    "# X_train_vec1, X_traintest_vec1, X_train_vec2, X_traintest_vec2, Y_train, Y_traintest = train_test_split(question1, question2,\\\n",
    "#                                                         train[\"is_duplicate\"],\\\n",
    "#                                                         train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_17 (InputLayer)            (None, 10)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_18 (InputLayer)            (None, 10)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)         (None, 10, 300)       1500000     input_17[0][0]                   \n",
      "                                                                   input_18[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                    (None, 256)           570368      embedding_11[0][0]               \n",
      "                                                                   embedding_11[1][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)      (None, 512)           0           lstm_9[0][0]                     \n",
      "                                                                   lstm_9[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 512)           0           concatenate_8[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 512)           2048        dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 100)           51300       batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 100)           0           dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 100)           400         dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 1)             101         batch_normalization_8[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 2,124,217\n",
      "Trainable params: 622,993\n",
      "Non-trainable params: 1,501,224\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Feed NN with Padded Sequences\n",
    "embedding_layer = lyr.Embedding(num_voc,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_seq_length,\n",
    "        trainable=False)\n",
    "lstm_layer = lyr.LSTM(lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "input1_tensor = lyr.Input(X_train_q1.shape[1:])\n",
    "input2_tensor = lyr.Input(X_train_q2.shape[1:])\n",
    "e1 = embedding_layer(input1_tensor)\n",
    "e2 = embedding_layer(input2_tensor)\n",
    "lstm1 = lstm_layer(e1)\n",
    "lstm2 = lstm_layer(e2)\n",
    "#e1 = lyr.Embedding(output_dim=10, input_dim=300, input_length=1)(input1_tensor) # Input Dimension = max_features of CountVec\n",
    "#e2 = lyr.Embedding(output_dim=10, input_dim=300, input_length=1)(input2_tensor)\n",
    "#lstm1 = lyr.LSTM(lstm_units, activation='tanh')(e1)\n",
    "#lstm2 = lyr.LSTM(lstm_units, activation='tanh')(e2)\n",
    "# Feed input1 and input2 seperately to LSTM:\n",
    "merge_layer = lyr.concatenate([lstm1, lstm2])  ### MERGE LAYER\n",
    "merge_layer = lyr.Dropout(rate_drop_dense)(merge_layer)\n",
    "merge_layer = lyr.BatchNormalization()(merge_layer)\n",
    "dense_layer = lyr.Dense(num_dense, activation='relu')(merge_layer) # DENSE LAYER\n",
    "dense_layer = lyr.Dropout(rate_drop_dense)(dense_layer)\n",
    "dense_layer = lyr.BatchNormalization()(dense_layer)\n",
    "output_layer = lyr.Dense(1, activation='sigmoid')(dense_layer)\n",
    "model = Model([input1_tensor, input2_tensor], output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer = Embedding(nb_words,\n",
    "#         EMBEDDING_DIM,\n",
    "#         weights=[embedding_matrix],\n",
    "#         input_length=MAX_SEQUENCE_LENGTH,\n",
    "#         trainable=False)\n",
    "# lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "# sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "# embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "# x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "# sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "# embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "# y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "# merged = concatenate([x1, y1])\n",
    "# merged = Dropout(rate_drop_dense)(merged)\n",
    "# merged = BatchNormalization()(merged)\n",
    "\n",
    "# merged = Dense(num_dense, activation=act)(merged)\n",
    "# merged = Dropout(rate_drop_dense)(merged)\n",
    "# merged = BatchNormalization()(merged)\n",
    "\n",
    "# preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/14\n",
      "24s - loss: 0.7648 - val_loss: 0.6629\n",
      "Epoch 2/14\n",
      "27s - loss: 0.6661 - val_loss: 0.6623\n",
      "Epoch 3/14\n",
      "29s - loss: 0.6380 - val_loss: 0.6589\n",
      "Epoch 4/14\n",
      "31s - loss: 0.6156 - val_loss: 0.6477\n",
      "Epoch 5/14\n",
      "29s - loss: 0.5936 - val_loss: 0.6375\n",
      "Epoch 6/14\n",
      "32s - loss: 0.5743 - val_loss: 0.6273\n",
      "Epoch 7/14\n",
      "35s - loss: 0.5726 - val_loss: 0.6222\n",
      "Epoch 8/14\n",
      "32s - loss: 0.5658 - val_loss: 0.6130\n",
      "Epoch 9/14\n",
      "37s - loss: 0.5544 - val_loss: 0.6051\n",
      "Epoch 10/14\n",
      "38s - loss: 0.5449 - val_loss: 0.5957\n",
      "Epoch 11/14\n",
      "34s - loss: 0.5383 - val_loss: 0.5913\n",
      "Epoch 12/14\n",
      "31s - loss: 0.5318 - val_loss: 0.5892\n",
      "Epoch 13/14\n",
      "30s - loss: 0.5267 - val_loss: 0.5990\n",
      "Epoch 14/14\n",
      "33s - loss: 0.5164 - val_loss: 0.6110\n",
      "[0]\ttrain-logloss:0.66668\tval-logloss:0.678459\n",
      "Multiple eval metrics have been passed: 'val-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-logloss hasn't improved in 10 rounds.\n",
      "[10]\ttrain-logloss:0.492024\tval-logloss:0.607801\n",
      "[20]\ttrain-logloss:0.403336\tval-logloss:0.5943\n",
      "[30]\ttrain-logloss:0.349715\tval-logloss:0.596078\n",
      "Stopping. Best iteration:\n",
      "[23]\ttrain-logloss:0.383214\tval-logloss:0.5941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FIT --> Train NN weights\n",
    "model.fit([X_train_q1, X_train_q2], Y_train, \n",
    "          validation_data=([X_traintest_q1, X_traintest_q2], Y_traintest), \n",
    "          batch_size=128, epochs=14, verbose=2)\n",
    "\n",
    "# (1) Take Features from Merge Layer and(2) feed to Classifier (XGBoost)\n",
    "#(1)\n",
    "mergeNN = Model([input1_tensor, input2_tensor], merge_layer)\n",
    "mergeNN.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "F_train = mergeNN.predict([X_train_q1, X_train_q1], batch_size=128)\n",
    "F_traintest = mergeNN.predict([X_traintest_q1, X_traintest_q2], batch_size=128)\n",
    "F_test = mergeNN.predict([X_test_q1, X_test_q2], batch_size=128)\n",
    "\n",
    "#(2)\n",
    "dTrain = xgb.DMatrix(F_train, label=Y_train)\n",
    "dTraintest = xgb.DMatrix(F_traintest, label=Y_traintest)\n",
    "dTest = xgb.DMatrix(F_test)\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'booster': 'gbtree',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.1, \n",
    "    'max_depth': 9,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 1 / F_train.shape[1]**0.5,\n",
    "    'min_child_weight': 5,\n",
    "    'silent': 1\n",
    "}\n",
    "bst = xgb.train(xgb_params, dTrain, 1000,  [(dTrain,'train'), (dTraintest,'val')], \n",
    "                verbose_eval=10, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Submission File\n"
     ]
    }
   ],
   "source": [
    "# PREDICT Submission File\n",
    "dTest = xgb.DMatrix(F_test)\n",
    "df_sub = pd.DataFrame({\n",
    "        'test_id': test_idxs,\n",
    "        'is_duplicate': bst.predict(dTest, ntree_limit=bst.best_ntree_limit)\n",
    "    }).set_index('test_id')\n",
    "\n",
    "\n",
    "print(\"Create Submission File\")\n",
    "df_sub.to_csv('newsub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_stops():\n",
    "\tprint \"Creating stopwords\"\n",
    "\tfrom nltk.corpus import stopwords\n",
    "\torig_stops = set(stopwords.words(\"english\"))\n",
    "\tnot_stops = set([\"what\", \"how\", \"who\"]) # add more if you like\n",
    "\tstops = orig_stops - not_stops\t\t# Set difference\n",
    "\treturn stops\n",
    "\n",
    "def create_padded_seqs(texts, mycv,max_seq_length):\n",
    "    seqs = texts.apply(text2ints, cv=mycv)\n",
    "    return pad_sequences(seqs, maxlen=max_seq_length)\n",
    "\n",
    "def text2ints(text, cv):\n",
    "    other_index = len(cv.vocabulary_)\n",
    "    intseq = []\n",
    "    mysplit = text.split(\" \")\n",
    "    filter(lambda x: x!=\"\", mysplit)\n",
    "    for word in mysplit:\n",
    "        intseq.append(cv.vocabulary_[word]) if word in cv.vocabulary_ else other_index\n",
    "    return intseq\n",
    "\n",
    "def load_train():\n",
    "    print(\"LOAD TRAIN DATA\")\n",
    "    train = pd.read_csv(Train_Path)\n",
    "    train['question1'].fillna('', inplace=True)\n",
    "    train['question2'].fillna('', inplace=True)\n",
    "    train = cleanquestions(train)\n",
    "    return train\n",
    "def load_test():\n",
    "    print(\"LOAD TEST DATA\")\n",
    "    test = pd.read_csv(Test_Path)\n",
    "    test['question1'].fillna('', inplace=True)\n",
    "    test['question2'].fillna('', inplace=True)\n",
    "    test = cleanquestions(test)\n",
    "    return test\n",
    "\n",
    "def cleanreview(review):\n",
    "    STOPS = [\"\"]\n",
    "    review = re.sub(\"[^a-z]\", \" \", review.lower()) # letters only\n",
    "    review = review.split(\" \")\n",
    "    meaningful_words = [w for w in review if not w in STOPS]\n",
    "    sentence = \" \".join(meaningful_words)\n",
    "    return sentence\n",
    "    \n",
    "def cleanquestions(df):\n",
    "\n",
    "    question1 = df.question1.apply(lambda x: cleanreview(x))\n",
    "    question2 = df.question2.apply(lambda x: cleanreview(x))\n",
    "    df.loc[:, 'question1'] = pd.Series(question1, index=df.index)\n",
    "    df.loc[:, 'question2'] = pd.Series(question1, index=df.index)\n",
    "    return df\n",
    "\n",
    "def splitData(ratio = 0.7): # ratio = train_set, between 0-1.0, default 0.7\n",
    "    print(\"SPLIT DATA\")\n",
    "    #time.sleep(1)\n",
    "    train = load_train()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(train.loc[:,[\"question1\", \"question2\"]],\\\n",
    "                                                        train[\"is_duplicate\"],\\\n",
    "                                                        train_size=0.7, random_state=42)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "def trainData():\n",
    "    print(\"LOAD TRAIN DATA FOR PREDICT AND SUBMIT\")\n",
    "    train = load_train()\n",
    "    X = train.loc[:, [\"question1\", \"question2\"]]\n",
    "    Y = train.loc[:][\"is_duplicate\"]\n",
    "    return X,Y\n",
    "def testData():\n",
    "    test = load_test()\n",
    "    X_test = test.loc[:,[\"question1\", \"question2\"]]\n",
    "    X_ids = test.loc[:][\"test_id\"]\n",
    "    return X_test, X_ids\n",
    "\n",
    "def rprint(str): # Next print overwrites this, i.e use for indicate progress\n",
    "\tsys.stdout.write(\"PROCESSING: \" + str + \"            \\r\")\n",
    "\tsys.stdout.flush()\n",
    "\n",
    "\n",
    "# serializes data and saves in a (somewhat) compressed format\n",
    "def save_zipped_pickle(obj, filename, protocol=-1):\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        cPickle.dump(obj, f, protocol)\n",
    "        f.close()\n",
    "\n",
    "# loads compressed data and restores original state\n",
    "def load_zipped_pickle(filename):\t# loads and unpacks\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = cPickle.load(f)\n",
    "        f.close()\n",
    "        return loaded_object   \n",
    "    \n",
    "    \n",
    "class BoW:\n",
    "    name = \"BoW\"\n",
    "    cv = 0\n",
    "\n",
    "    def fit(self, X, num_vocab):\n",
    "        corpus = pd.concat([X.question1, X.question2])\n",
    "        self.cv = CountVectorizer(analyzer='word',min_df = 0,\n",
    "        max_features=num_vocab, ngram_range=(1,1), preprocessor=None, stop_words=None,\n",
    "        tokenizer=None)\n",
    "        print(\"FIT CORPUS ON COUNT VECTORIZER\")\n",
    "        self.cv.fit(corpus)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.cv.transform(X)\n",
    "    def getVectorizer(self):\n",
    "        return self.cv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
