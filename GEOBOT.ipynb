{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "#import urllib PYTHON 2 Sorry :D\n",
    "import urlparse\n",
    "import config\n",
    "from nlp import *\n",
    "import os.path\n",
    "#from preproc import preproc --> Because of Memory\n",
    "import pandas as pd\n",
    "# for Word2Vec model Load\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "URL = \"https://api.telegram.org/bot{}/\".format(config.TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////////\n",
    "def get_url(url):\n",
    "    response = requests.get(url)\n",
    "    content = response.content.decode(\"utf8\")\n",
    "    return content\n",
    "\n",
    "\n",
    "def get_json_from_url(url):\n",
    "    content = get_url(url)\n",
    "    js = json.loads(content)\n",
    "    return js\n",
    "\n",
    "\n",
    "def get_updates(offset=None):\n",
    "    url = URL + \"getUpdates?timeout=100\"\n",
    "    print(\"updates\")\n",
    "    if offset:\n",
    "    \turl += \"&offset={}\".format(offset)\n",
    "    js = get_json_from_url(url)\n",
    "    return js\n",
    "\n",
    "def get_last_chat_id_and_text(updates):\n",
    "    num_updates = len(updates[\"result\"])\n",
    "    last_update = num_updates - 1\n",
    "    text = updates[\"result\"][last_update][\"message\"][\"text\"]\n",
    "    chat_id = updates[\"result\"][last_update][\"message\"][\"chat\"][\"id\"]\n",
    "    return (text, chat_id)\n",
    "\n",
    "def send_message(text, chat_id, reply_markup=None):\n",
    "    #text = urllib.parse.quote_plus(text) for Python 2\n",
    "    #text = urlparse.urlparse(text)\n",
    "    url = URL + \"sendMessage?text={}&chat_id={}&parse_mode=Markdown\".format(text, chat_id)\n",
    "    if reply_markup:\n",
    "        url += \"&reply_markup={}\".format(reply_markup)\n",
    "    get_url(url)\n",
    "\n",
    "def build_keyboard(items):\n",
    "    keyboard = [[item] for item in items]\n",
    "    reply_markup = {\"keyboard\":keyboard, \"one_time_keyboard\": True}\n",
    "    return json.dumps(reply_markup)\n",
    "    \n",
    "def get_last_update_id(updates):\n",
    "    update_ids = []\n",
    "    for update in updates[\"result\"]:\n",
    "        update_ids.append(int(update[\"update_id\"]))\n",
    "    return max(update_ids)\n",
    "\n",
    "def handle_updates(updates, nlp):\n",
    "    for update in updates[\"result\"]:\n",
    "        try:\n",
    "            text = update[\"message\"][\"text\"]\n",
    "            chat = update[\"message\"][\"chat\"][\"id\"]\n",
    "            #items = db.get_items(chat)\n",
    "            message = nlp.respond(text)\n",
    "            print(\"response = \" + message)\n",
    "            send_message(message, chat)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "def load_zipped_pickle(filename):\t# loads and unpacks\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = cPickle.load(f)\n",
    "        f.close()\n",
    "        return loaded_object\n",
    "\n",
    "def main():\n",
    "    last_update_id = None\n",
    "    #if not os.path.isfile(\"proc_data/proc_data.csv\"): # Outcommented because of memory\n",
    "    #    preproc()\n",
    "    df = pd.DataFrame.from_csv(\"proc_data/proc_data.csv\")\n",
    "    nlp = NLP(df)\n",
    "    print(\"The bot is now active\")\n",
    "    while True:\n",
    "        updates = get_updates(last_update_id)\n",
    "        if len(updates[\"result\"]) > 0:\n",
    "            last_update_id = get_last_update_id(updates) + 1\n",
    "            handle_updates(updates, nlp)\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.keyedvectors:loading projection weights from ./model/GoogleNews-vectors-negative300.bin\n",
      "INFO:gensim.models.keyedvectors:loaded (3000000, 300) matrix from ./model/GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "# Split the Main Function to not always load again\n",
    "# Stops at 6.6 GB for me :D\n",
    "\n",
    "EMBEDDING_FILE = './model/GoogleNews-vectors-negative300.bin'\n",
    "w2vmodel = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "        binary=True)\n",
    "description_vecs = load_zipped_pickle('./descriptions_vec.npy')\n",
    "#save_zipped_pickle(descriptions_vec,'descriptions_vec.npy')\n",
    "last_update_id = None\n",
    "#if not os.path.isfile(\"proc_data/proc_data.csv\"): # Outcommented because of memory\n",
    "#    preproc()\n",
    "df = pd.DataFrame.from_csv(\"proc_data/proc_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELOAD CHANGES IN NLP FILE --> For Debugging\n",
    "import nlp\n",
    "reload(nlp)\n",
    "from nlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9d466d169710>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# nlp = NLP(df, word2vecmodel, descriptions_vec) ---> Don't want to safe that several times so I pass it directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2vmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Loaded Word2Vec model + Descriptions_vec\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The bot is now active\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "nlp = NLP(df, w2vmodel, description_vecs)\n",
    "print \"Loaded Word2Vec model + Descriptions_vec\"\n",
    "print(\"The bot is now active\")\n",
    "\n",
    "while True:\n",
    "        updates = get_updates(last_update_id)\n",
    "        if len(updates[\"result\"]) > 0:\n",
    "            last_update_id = get_last_update_id(updates) + 1\n",
    "            handle_updates(updates, nlp)\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
